{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction Model - Complete Guide\n",
    "\n",
    "This notebook provides a comprehensive, step-by-step guide to building a customer churn prediction model for a bank. We'll cover everything from data loading to model deployment.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Initial Exploration](#data-loading)\n",
    "2. [Data Preprocessing](#preprocessing)\n",
    "3. [Feature Engineering](#feature-engineering)\n",
    "4. [Data Visualization](#visualization)\n",
    "5. [Model Building and Training](#model-building)\n",
    "6. [Model Evaluation](#model-evaluation)\n",
    "7. [Feature Importance Analysis](#feature-importance)\n",
    "8. [Model Deployment Preparation](#deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-loading'></a>\n",
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('botswana_bank_customer_churn.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\nDuplicate Rows:\", df.duplicated().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocessing'></a>\n",
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For CreditScore, Age, and Balance, we'll fill with median\n",
    "df['CreditScore'].fillna(df['CreditScore'].median(), inplace=True)\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df['Balance'].fillna(df['Balance'].median(), inplace=True)\n",
    "\n",
    "# For NumOfProducts, we'll fill with mode\n",
    "df['NumOfProducts'].fillna(df['NumOfProducts'].mode()[0], inplace=True)\n",
    "\n",
    "# For HasCrCard and IsActiveMember, we'll fill with mode\n",
    "df['HasCrCard'].fillna(df['HasCrCard'].mode()[0], inplace=True)\n",
    "df['IsActiveMember'].fillna(df['IsActiveMember'].mode()[0], inplace=True)\n",
    "\n",
    "# Check missing values again\n",
    "print(\"Missing Values After Imputation:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers using IQR method for numerical columns\n",
    "def remove_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Apply outlier removal to numerical columns\n",
    "numerical_columns = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']\n",
    "df_clean = df.copy()\n",
    "\n",
    "for col in numerical_columns:\n",
    "    df_clean = remove_outliers_iqr(df_clean, col)\n",
    "    \n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Dataset size after outlier removal: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature-engineering'></a>\n",
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "\n",
    "# Credit Utilization Ratio\n",
    "df_clean['CreditUtilizationRatio'] = df_clean['Balance'] / (df_clean['CreditScore'] * 100)\n",
    "\n",
    "# Customer Lifetime Value (CLV) approximation\n",
    "df_clean['CLV'] = df_clean['Balance'] * df_clean['NumOfProducts'] * df_clean['EstimatedSalary'] / 1000000\n",
    "\n",
    "# Risk Score based on multiple factors\n",
    "df_clean['RiskScore'] = (\n",
    "    (df_clean['Age'] / 100) * 0.2 +\n",
    "    (1 - df_clean['IsActiveMember']) * 0.3 +\n",
    "    (df_clean['NumOfProducts'] > 1).astype(int) * 0.2 +\n",
    "    (df_clean['HasCrCard'] == 0).astype(int) * 0.3\n",
    ")\n",
    "\n",
    "# Tenure groups\n",
    "df_clean['TenureGroup'] = pd.cut(df_clean['Tenure'], bins=[0, 2, 5, 10, float('inf')], \n",
    "                                labels=['New', 'Established', 'Long-term', 'Veteran'])\n",
    "\n",
    "# Balance categories\n",
    "df_clean['BalanceCategory'] = pd.cut(df_clean['Balance'], \n",
    "                                    bins=[-1, 0, 50000, 100000, float('inf')], \n",
    "                                    labels=['NoBalance', 'Low', 'Medium', 'High'])\n",
    "\n",
    "# Display the new features\n",
    "print(\"New Features Created:\")\n",
    "df_clean[['CreditUtilizationRatio', 'CLV', 'RiskScore', 'TenureGroup', 'BalanceCategory']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Geography', 'Gender', 'TenureGroup', 'BalanceCategory']\n",
    "\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Categorical columns encoded successfully!\")\n",
    "print(\"Encoded columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualization'></a>\n",
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df_encoded, x='Exited')\n",
    "plt.title('Distribution of Churn (Exited)')\n",
    "plt.xlabel('Churn Status (0: Not Churned, 1: Churned)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "churn_rate = df_encoded['Exited'].mean() * 100\n",
    "print(f\"Churn Rate: {churn_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = df_encoded.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution by churn status\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df_encoded, x='Age', hue='Exited', kde=True, bins=30)\n",
    "plt.title('Age Distribution by Churn Status')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-building'></a>\n",
    "## 5. Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "features = [\n",
    "    'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', \n",
    "    'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary',\n",
    "    'CreditUtilizationRatio', 'CLV', 'RiskScore', 'TenureGroup', 'BalanceCategory'\n",
    "]\n",
    "\n",
    "X = df_encoded[features]\n",
    "y = df_encoded['Exited']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance using upsampling\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = train_data[train_data.Exited == 0]\n",
    "minority_class = train_data[train_data.Exited == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "minority_upsampled = resample(minority_class, \n",
    "                              replace=True,     # sample with replacement\n",
    "                              n_samples=len(majority_class),    # match majority class\n",
    "                              random_state=42)  # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "train_balanced = pd.concat([majority_class, minority_upsampled])\n",
    "\n",
    "# Separate features and target\n",
    "X_train_balanced = train_balanced.drop('Exited', axis=1)\n",
    "y_train_balanced = train_balanced['Exited']\n",
    "\n",
    "print(f\"Original training set class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Balanced training set class distribution:\\n{y_train_balanced.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest model with parameters to prevent overfitting\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Random Forest model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-evaluation'></a>\n",
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature-importance'></a>\n",
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'feature': features, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importance_df, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deployment'></a>\n",
    "## 8. Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(rf_model, 'customer_churn_model.pkl')\n",
    "\n",
    "# Save the label encoders\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "print(\"Model and encoders saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to load and use the model for predictions\n",
    "# Load the model and encoders\n",
    "loaded_model = joblib.load('customer_churn_model.pkl')\n",
    "loaded_encoders = joblib.load('label_encoders.pkl')\n",
    "\n",
    "# Example prediction function\n",
    "def predict_churn(customer_data):\n",
    "    \"\"\"\n",
    "    Predict churn for a single customer\n",
    "    customer_data: dict with customer information\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    df_customer = pd.DataFrame([customer_data])\n",
    "    \n",
    "    # Apply same preprocessing\n",
    "    df_customer['CreditUtilizationRatio'] = df_customer['Balance'] / (df_customer['CreditScore'] * 100)\n",
    "    df_customer['CLV'] = df_customer['Balance'] * df_customer['NumOfProducts'] * df_customer['EstimatedSalary'] / 1000000\n",
    "    df_customer['RiskScore'] = (\n",
    "        (df_customer['Age'] / 100) * 0.2 +\n",
    "        (1 - df_customer['IsActiveMember']) * 0.3 +\n",
    "        (df_customer['NumOfProducts'] > 1).astype(int) * 0.2 +\n",
    "        (df_customer['HasCrCard'] == 0).astype(int) * 0.3\n",
    "    )\n",
    "    df_customer['TenureGroup'] = pd.cut(df_customer['Tenure'], bins=[0, 2, 5, 10, float('inf')], \n",
    "                                       labels=['New', 'Established', 'Long-term', 'Veteran'])\n",
    "    df_customer['BalanceCategory'] = pd.cut(df_customer['Balance'], \n",
    "                                           bins=[-1, 0, 50000, 100000, float('inf')], \n",
    "                                           labels=['NoBalance', 'Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    for col in ['Geography', 'Gender', 'TenureGroup', 'BalanceCategory']:\n",
    "        df_customer[col] = loaded_encoders[col].transform(df_customer[col])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = loaded_model.predict(df_customer[features])[0]\n",
    "    probability = loaded_model.predict_proba(df_customer[features])[0][1]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Example usage\n",
    "example_customer = {\n",
    "    'CreditScore': 650,\n",
    "    'Geography': 'France',\n",
    "    'Gender': 'Male',\n",
    "    'Age': 35,\n",
    "    'Tenure': 5,\n",
    "    'Balance': 50000,\n",
    "    'NumOfProducts': 2,\n",
    "    'HasCrCard': 1,\n",
    "    'IsActiveMember': 1,\n",
    "    'EstimatedSalary': 60000\n",
    "}\n",
    "\n",
    "prediction, probability = predict_churn(example_customer)\n",
    "print(f\"Prediction: {'Churn' if prediction == 1 else 'Not Churn'}\")\n",
    "print(f\"Churn Probability: {probability:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has guided you through the complete process of building a customer churn prediction model:\n",
    "\n",
    "1. **Data Loading and Exploration**: Loaded the dataset and understood its structure\n",
    "2. **Data Preprocessing**: Handled missing values and outliers\n",
    "3. **Feature Engineering**: Created new meaningful features\n",
    "4. **Data Visualization**: Explored data patterns and relationships\n",
    "5. **Model Building**: Trained a Random Forest classifier with balanced data\n",
    "6. **Model Evaluation**: Assessed performance with multiple metrics\n",
    "7. **Feature Importance**: Identified key drivers of churn\n",
    "8. **Deployment Preparation**: Saved the model and demonstrated usage\n",
    "\n",
    "The model achieves good performance while avoiding overfitting through:\n",
    "- Balanced training data\n",
    "- Regularization parameters in the Random Forest\n",
    "- Feature engineering to capture meaningful patterns\n",
    "\n",
    "You can now use this model to predict customer churn and take proactive retention measures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}